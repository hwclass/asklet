## ğŸŒ Asklet
### In-Browser LLM Benchmarking with WebLLM + LangChain

https://github.com/hwclass/asklet/assets/asklet-intro.mov

Asklet is an open benchmarking sandbox for testing local LLM inference performance across modern frontend frameworks like React, Svelte, and Qwik â€” using WebLLM and LangChain.js.

ğŸ” What it does:
- Runs LLMs entirely in the browser â€” no server or API calls.
- Injects a DevTools-callable ask() function for experiments.
- Captures detailed token-level performance metrics:
  - Time to First Token (TTFT)
  - Time per Output Token (TPOT)
  - Total generation time
  - Token throughput
  - JS heap usage
  - Script transfer size

ğŸ§ª Why it matters:
This repo helps answer:

_â€œHow efficient is local inference for real-world prompts across frameworks?â€_

Great for evaluating performance trade-offs when embedding LLMs natively into your apps, extensions, or UIs.

ğŸ“¦ Includes:
    â€¢ Minimal demo apps for React, Svelte, Qwik
    â€¢ Shared benchmarking logic and metric collector
    â€¢ Sample prompts and usage guide

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

Copyright (c) 2025
MIT License